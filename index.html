<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Tree: 머신러닝의 기초</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <header>
        <h1>Decision Tree Learning</h1>
        <p>데이터 속에서 규칙을 찾아내는 나무</p>
    </header>

    <div class="container">
        <nav class="sidebar">
            <h3>Contents</h3>
            <ul>
                <li><a href="#sec1">1. 의사결정 나무란?</a></li>
                <li><a href="#sec2">2. 역사와 유래</a></li>
                <li><a href="#sec3">3. 기본 구조</a></li>
                <li><a href="#sec4">4. 핵심 개념: 불순도</a></li>
                <li><a href="#sec5">5. 수학: 엔트로피</a></li>
                <li><a href="#sec6">6. 수학: 지니 계수</a></li>
                <li><a href="#sec7">7. 정보 이득 (IG)</a></li>
                <li><a href="#sec8">8. 가지치기 (Pruning)</a></li>
                <li><a href="#sec9">9. 장점과 단점</a></li>
                <li><a href="#sec10">10. Python 구현</a></li>
            </ul>
        </nav>

        <main class="content">
            
            <section id="sec1">
                <h2>1. 의사결정 나무(Decision Tree)란?</h2>
                <p>의사결정 나무는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내는 머신러닝 알고리즘입니다. 마치 스무고개 놀이처럼 "예/아니오" 질문을 반복하며 정답을 찾아가는 구조를 가집니다.</p>
                <p>분류(Classification)와 회귀(Regression) 문제 모두에 사용할 수 있는 지도 학습 모델입니다.</p>
            </section>

            <section id="sec2">
                <h2>2. 역사와 유래 (History)</h2>
                <p>의사결정 나무의 개념은 1960년대부터 시작되었습니다.</p>
                <ul>
                    <li><strong>1966년 (CLS):</strong> Earl B. Hunt 등이 Concept Learning System을 개발하여 기초를 마련했습니다.</li>
                    <li><strong>1979년 (ID3):</strong> Ross Quinlan이 엔트로피 기반의 ID3 알고리즘을 발표하며 널리 알려졌습니다.</li>
                    <li><strong>1984년 (CART):</strong> Leo Breiman 등이 Classification and Regression Tree를 발표하여 지니 계수를 도입했습니다.</li>
                    <li><strong>1993년 (C4.5):</strong> Ross Quinlan이 ID3를 개선하여 연속형 데이터 처리를 가능하게 했습니다.</li>
                </ul>
            </section>

            <section id="sec3">    
                <h2>3. 기본 구조 (Structure)</h2>
                <p>나무가 뒤집혀 있는 모양으로, 뿌리(Root)에서 시작해 잎(Leaf)으로 끝납니다. 각 부분의 역할은 다음과 같습니다.</p>

                <figure class="diagram-container">
                    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Decision_Tree.jpg/640px-Decision_Tree.jpg" alt="의사결정 나무의 기본 구조: 루트 노드, 내부 노드, 리프 노드">
                    <figcaption>그림 1: 의사결정 나무의 기본 구조 (출처: Wikimedia Commons)</figcaption>
                </figure>
                
                <ul>
                    <li><strong>뿌리 노드 (Root Node):</strong> 트리의 시작점입니다. 가장 첫 번째 질문이 위치하며, 입력된 모든 데이터를 포함하고 있습니다.</li>
                    <li><strong>내부 노드 (Internal Node):</strong> 조건에 따라 데이터가 분기(Split)되는 지점입니다. '스무고개'의 질문에 해당하며, 각 분기점마다 특정 기준(특성)에 따라 데이터가 나뉩니다.</li>
                    <li><strong>잎 노드 (Leaf Node):</strong> 더 이상 분할되지 않는 트리의 끝점입니다. 최종적인 예측값(클래스 레이블 또는 회귀 값)을 결정하는 곳입니다.</li>
                </ul>
            </section>

            <section id="sec4">
                <h2>4. 핵심 개념: 불순도 (Impurity)</h2>
                <p>트리를 나눌 때 가장 중요한 기준은 "어떻게 나누어야 데이터를 가장 잘 구분할 수 있는가?"입니다. 이때 <strong>불순도(Impurity)</strong>가 낮아지는 방향으로 데이터를 분할합니다.</p>
                <blockquote>
                    불순도가 낮다 = 데이터가 균일하게 섞여 있다 (순수하다).<br>
                    불순도가 높다 = 여러 클래스가 마구 섞여 있다.
                </blockquote>
            </section>

            <section id="sec5">
                <h2>5. 수학 공식 1: 엔트로피 (Entropy)</h2>
                <p>데이터 집합의 혼잡도를 의미합니다. 확률 변수 $p_i$가 특정 클래스에 속할 확률일 때, 엔트로피 $H(S)$는 다음과 같습니다.</p>
                <div class="formula">
                    $$ H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i) $$
                </div>
                <p>값이 0이면 완벽하게 분류된 상태이고, 1에 가까울수록 데이터가 무질서하게 섞인 상태입니다.</p>
            </section>

            <section id="sec6">
                <h2>6. 수학 공식 2: 지니 계수 (Gini Impurity)</h2>
                <p>CART 알고리즘에서 주로 사용하며, 계산이 엔트로피보다 빠릅니다. 데이터셋에서 임의로 두 개의 샘플을 뽑았을 때 서로 다른 클래스일 확률을 의미합니다.</p>
                <div class="diagram-placeholder">
                    
                </div>
                <div class="formula">
                    $$ G = 1 - \sum_{i=1}^{c} p_i^2 $$
                </div>
                <p>지니 계수가 0이면 모든 데이터가 하나의 클래스에 속함을 의미합니다.</p>
            </section>

            <section id="sec7">
                <h2>7. 정보 이득 (Information Gain)</h2>
                <p>부모 노드와 분할 후 자식 노드들 간의 불순도 차이를 <strong>정보 이득</strong>이라고 합니다.</p>
                <div class="formula">
                    $$ IG(S, A) = H(S) - \sum \frac{|S_v|}{|S|} H(S_v) $$
                </div>
                <p>Decision Tree는 이 정보 이득(IG)이 최대화되는 방향(불순도가 가장 많이 감소하는 방향)으로 가지를 칩니다.</p>
            </section>

            <section id="sec8">
                <h2>8. 가지치기 (Pruning)</h2>
                <p>트리가 너무 깊어지면 학습 데이터만 완벽하게 외워버리는 <strong>과적합(Overfitting)</strong>이 발생합니다. 이를 막기 위해 트리의 성장을 제한하는 것을 가지치기라고 합니다.</p>
                <ul>
                    <li><strong>Pre-pruning:</strong> 트리를 만들 때 깊이(max_depth)나 노드 수에 제한을 둡니다.</li>
                    <li><strong>Post-pruning:</strong> 트리를 완성한 후 중요하지 않은 가지를 잘라냅니다.</li>
                </ul>
            </section>

            <section id="sec9">
                <h2>9. 장점과 단점</h2>
                <table>
                    <tr>
                        <th>장점 (Pros)</th>
                        <th>단점 (Cons)</th>
                    </tr>
                    <tr>
                        <td>이해와 해석이 쉽다 (시각화 용이)</td>
                        <td>과적합(Overfitting) 되기 쉽다</td>
                    </tr>
                    <tr>
                        <td>데이터 전처리(스케일링 등)가 거의 필요 없다</td>
                        <td>데이터의 작은 변화에도 트리가 크게 바뀔 수 있다</td>
                    </tr>
                    <tr>
                        <td>범주형, 연속형 데이터 모두 처리 가능</td>
                        <td>경계면이 수직/수평으로만 형성된다</td>
                    </tr>
                </table>
            </section>

            <section id="sec10">
                <h2>10. Python Code Example</h2>
                <p>Scikit-learn 라이브러리를 사용한 간단한 구현 예제입니다.</p>
                <pre><code>
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 1. 데이터 로드
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2
)

# 2. 모델 생성 및 학습 (가지치기: max_depth=3)
dt_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_clf.fit(X_train, y_train)

# 3. 예측 및 정확도 확인
score = dt_clf.score(X_test, y_test)
print(f"정확도: {score:.4f}")
                </code></pre>
            </section>

        </main>
    </div>

    <footer>
        <p>&copy; 2025 ML Blog Project by Student. All rights reserved.</p>
    </footer>

</body>
</html>