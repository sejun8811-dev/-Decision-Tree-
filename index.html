<!DOCTYPE html>
<html lang="ko">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Tree: 머신러닝의 기초</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <header>
        <h1>Decision Tree Learning</h1>
        <p>데이터 속에서 규칙을 찾아내는 나무</p>
    </header>

    <div class="container">
        <nav class="sidebar">
            <h3>Contents</h3>
            <ul>
                <li><a href="#sec1">1. 의사결정 나무란?</a></li>
                <li><a href="#sec2">2. 역사와 유래</a></li>
                <li><a href="#sec3">3. 기본 구조</a></li>
                <li><a href="#sec4">4. 핵심 개념: 불순도</a></li>
                <li><a href="#sec5">5. 수학: 엔트로피</a></li>
                <li><a href="#sec6">6. 수학: 지니 계수</a></li>
                <li><a href="#sec7">7. 정보 이득 (IG)</a></li>
                <li><a href="#sec8">8. 가지치기 (Pruning)</a></li>
                <li><a href="#sec9">9. 장점과 단점</a></li>
                <li><a href="#sec10">10. Python 구현</a></li>
            </ul>
        </nav>

        <main class="content">

            <section id="sec1">
                <h2>1. 의사결정 나무(Decision Tree)란?</h2>

                <p>
                    **의사결정 나무(Decision Tree)**는 데이터에 내재된 규칙을 학습하여, 나무가 가지를 치듯(Tree-structure) 규칙을 조합해 나가는 **지도
                    학습(Supervised Learning)** 모델입니다.
                    복잡한 데이터 집단에서 가장 설명력이 높은 기준을 찾아 데이터를 나누고, 이 과정을 반복하여 최종적인 예측을 수행합니다.
                    마치 탐정이 단서를 하나씩 모아 범인을 좁혀가는 과정과 비슷하며, 결과 모델이 나무를 뒤집어 놓은 형태(뿌리가 위, 잎이 아래)와 같다고 하여 '의사결정 나무'라는 이름이
                    붙었습니다.
                </p>

                <h3>화이트 박스(White Box) 모델</h3>
                <p>
                    최근 유행하는 딥러닝(Deep Learning)은 내부 동작 과정을 명확히 알기 어려운 '블랙 박스(Black Box)' 모델인 반면,
                    의사결정 나무는 예측 과정이 투명하게 공개되는 **'화이트 박스(White Box)'** 모델입니다.
                    "수입이 300만 원 이하이고, 신용등급이 5등급보다 낮아서 대출이 거절되었다"는 식으로,
                    왜(Why) 그런 결과가 나왔는지에 대한 **설명력(Interpretability)**과 **해석 가능성**이 매우 뛰어납니다.
                    이러한 특징 덕분에 판단의 근거가 필수적인 의료 진단, 금융권 신용 평가, 기업의 마케팅 타겟 분석 분야에서 여전히 핵심적인 도구로 사용됩니다.
                </p>

                <h3>작동 원리: 스무고개와 분할 정복</h3>
                <p>
                    이 알고리즘의 핵심 원리는 **'분할 정복(Divide and Conquer)'**입니다.
                    전체 데이터를 가장 잘 구분할 수 있는 첫 번째 질문(Root Node)을 던지고, 대답에 따라 데이터를 두 그룹으로 나눕니다.
                    나위어진 각 그룹에 대해 또다시 가장 좋은 질문을 찾아 나누는 과정을 반복합니다(Recursive Partitioning).
                </p>
                <p>
                    이해를 돕기 위해 **'오늘 점심 메뉴 고르기'**를 상상해 봅시다.
                </p>
                <ul>
                    <li><strong>첫 번째 질문:</strong> "국물이 있는가?" (Yes &rarr; 찌개/탕 그룹, No &rarr; 볶음/구이 그룹)</li>
                    <li><strong>두 번째 질문(Yes 그룹):</strong> "매운가?" (Yes &rarr; 김치찌개, No &rarr; 설렁탕)</li>
                    <li><strong>세 번째 질문(No 그룹):</strong> "면인가?" (Yes &rarr; 파스타, No &rarr; 비빔밥)</li>
                </ul>
                <p>
                    이처럼 질문을 거듭할수록 우리는 정답(Target Class)에 가까워지게 되며,
                    각 그룹이 한 가지 메뉴로만 통일될 때(불순도가 0이 될 때) 질문을 멈춥니다.
                </p>

                <h3>CART 알고리즘과 특징</h3>
                <p>
                    현대적인 의사결정 나무는 대부분 **CART(Classification And Regression Tree)** 알고리즘을 기반으로 합니다.
                    이는 범주형 데이터(Classification)뿐만 아니라, 주택 가격 예측과 같은 연속적인 숫자(Regression)도 예측할 수 있음을 의미합니다.
                    또한 데이터의 스케일링(Scaling)이나 정규화 같은 전처리가 거의 필요 없으며,
                    데이터 내의 **이상치(Outlier)**가 있어도 비교적 안정적으로 동작한다는 강점을 가집니다.
                </p>
            </section>

            <section id="sec2">
                <h2>2. 역사와 유래 (History)</h2>
                <p>의사결정 나무의 개념은 1960년대부터 시작되었습니다.</p>
                <ul>
                    <li><strong>1966년 (CLS):</strong> Earl B. Hunt 등이 Concept Learning System을 개발하여 기초를 마련했습니다.</li>
                    <li><strong>1979년 (ID3):</strong> Ross Quinlan이 엔트로피 기반의 ID3 알고리즘을 발표하며 널리 알려졌습니다.</li>
                    <li><strong>1984년 (CART):</strong> Leo Breiman 등이 Classification and Regression Tree를 발표하여 지니 계수를
                        도입했습니다.</li>
                    <li><strong>1993년 (C4.5):</strong> Ross Quinlan이 ID3를 개선하여 연속형 데이터 처리를 가능하게 했습니다.</li>
                </ul>
            </section>

            <section id="sec3">
                <h2>3. 기본 구조 (Structure)</h2>
                <p>나무가 뒤집혀 있는 모양으로, 뿌리(Root)에서 시작해 잎(Leaf)으로 끝납니다. 각 부분의 역할은 다음과 같습니다.</p>

                <figure class="diagram-container">
                    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Decision_Tree.jpg/640px-Decision_Tree.jpg"
                        alt="의사결정 나무의 기본 구조: 루트 노드, 내부 노드, 리프 노드">
                    <figcaption>그림 1: 의사결정 나무의 기본 구조 (출처: Wikimedia Commons)</figcaption>
                </figure>

                <ul>
                    <li><strong>뿌리 노드 (Root Node):</strong> 트리의 시작점입니다. 가장 첫 번째 질문이 위치하며, 입력된 모든 데이터를 포함하고 있습니다.</li>
                    <li><strong>내부 노드 (Internal Node):</strong> 조건에 따라 데이터가 분기(Split)되는 지점입니다. '스무고개'의 질문에 해당하며, 각 분기점마다
                        특정 기준(특성)에 따라 데이터가 나뉩니다.</li>
                    <li><strong>잎 노드 (Leaf Node):</strong> 더 이상 분할되지 않는 트리의 끝점입니다. 최종적인 예측값(클래스 레이블 또는 회귀 값)을 결정하는 곳입니다.
                    </li>
                </ul>
            </section>

            <section id="sec4">
                <h2>4. 핵심 개념: 불순도 (Impurity)</h2>

                <p>
                    의사결정 나무가 "어떤 질문으로 가지를 칠까?"를 고민할 때 사용하는 유일한 판단 기준, 그것이 바로 **불순도(Impurity)**입니다.
                    데이터 분석의 목표는 혼잡한 상태에서 질서 있는 상태로 나아가는 것인데, 불순도는 **'데이터가 얼마나 섞여 있는가'**를 나타내는 척도입니다.
                    반대 말로는 **순도(Purity)** 또는 동질성(Homogeneity)이라고 합니다.
                </p>

                <h3>직관적인 비유: 빨간 공과 파란 공</h3>
                <p>
                    상자 안에 **빨간 공 50개**와 **파란 공 50개**가 뒤섞여 있다고 상상해 봅시다.
                    이 상태에서 눈을 감고 공 하나를 꺼냈을 때, 그 공이 무슨 색일지 확신할 수 있을까요?
                    전혀 알 수 없습니다. 이것이 바로 **불순도가 가장 높은(최대) 상태**입니다. 정보의 불확실성이 극에 달한 상태죠.
                </p>

                <div class="diagram-container">

                    <figcaption>그림 2: 불순도가 높은 상태(좌)와 낮은 상태(우)의 비교</figcaption>
                </div>

                <p>
                    반면, 어떤 질문을 통해 공들을 나누었더니 한쪽 상자에는 **빨간 공만 100개** 모였다고 가정해 봅시다.
                    이 상자에서 공을 꺼내면 무조건 빨간색입니다. 예측이 100% 확실합니다.
                    이것이 **불순도가 0인 상태(Pure Node)**이며, 의사결정 나무가 도달하고자 하는 최종 목표입니다.
                </p>

                <h3>알고리즘의 목표: 불확실성의 감소</h3>
                <p>
                    Decision Tree 알고리즘(ID3, CART 등)은 부모 노드보다 자식 노드들의 불순도가 더 낮아지도록(순도가 높아지도록) 데이터를 분할합니다.
                    즉, <strong>"여러 클래스가 마구 섞인 상태"에서 "같은 클래스끼리 모인 상태"로</strong> 나아가는 과정입니다.
                </p>

                <ul>
                    <li><strong>High Impurity (높은 불순도):</strong> 데이터가 균일하지 않음. 여러 종류가 섞여 있어 예측이 어려움. (예: 50% vs 50%)
                    </li>
                    <li><strong>Low Impurity (낮은 불순도):</strong> 데이터가 균일함. 소수의 종류만 남아 있어 예측이 쉬움. (예: 90% vs 10%)</li>
                    <li><strong>Pure Node (순수 노드):</strong> 불순도가 0인 상태. 단 하나의 클래스만 남은 상태로, 이곳이 바로 나무의 끝인 <strong>리프
                            노드(Leaf Node)</strong>가 됩니다.</li>
                </ul>

                <p>
                    이러한 불순도를 숫자로 계산하기 위해 우리는 다음 챕터에서 다룰 **'엔트로피(Entropy)'**나 **'지니 계수(Gini Impurity)'** 같은 수학적 공식을 사용하게
                    됩니다.
                </p>
            </section>

            <section id="sec5">
                <h2>5. 수학 공식 1: 엔트로피 (Entropy)</h2>

                <p>
                    **엔트로피(Entropy)**는 본래 열역학에서 '물질의 무질서도'를 나타내는 개념이었으나, 1948년 클로드 섀넌(Claude Shannon)이 이를 **정보
                    이론(Information Theory)**으로 가져와 **'정보의 불확실성'**을 측정하는 척도로 정립했습니다.
                </p>
                <p>
                    머신러닝, 특히 의사결정 나무에서 엔트로피는 <strong>"데이터 집합 안에 서로 다른 클래스들이 얼마나 뒤섞여 있는가?"</strong>를 숫자로 표현하는 가장 중요한
                    도구입니다. 데이터가 엉망으로 섞여 있을수록(불확실할수록) 엔트로피 값은 커지고, 깔끔하게 정리될수록 작아집니다.
                </p>

                <h3>엔트로피 공식의 이해</h3>
                <p>
                    데이터 집합 $S$에 대해, 클래스 $i$에 속할 확률을 $p_i$라고 할 때, 엔트로피 $H(S)$는 다음과 같이 정의됩니다.
                </p>

                <div class="formula">
                    $$ H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i) $$
                </div>

                <ul>
                    <li><strong>마이너스(-) 기호의 이유:</strong> 확률($p_i$)은 0과 1 사이의 값(예: 0.5)입니다. 이를 로그($\log$) 취하면 음수가 나오기
                        때문에, 최종 값을 양수로 만들기 위해 앞에 마이너스를 붙입니다.</li>
                    <li><strong>밑이 2인 로그($\log_2$):</strong> 컴퓨터는 0과 1(비트)로 정보를 처리하므로, 정보량을 '비트(bit)' 단위로 표현하기 위해 밑을 2로
                        사용합니다.</li>
                </ul>


                <h3>계산 예시: 동전 던지기</h3>
                <p>
                    이 개념을 가장 쉽게 이해하기 위해 동전 던지기를 예로 들어보겠습니다. 앞면(Head)과 뒷면(Tail)이 나올 확률을 계산해 봅니다.
                </p>

                <h4>상황 A: 공정한 동전 (완전한 혼잡)</h4>
                <p>
                    앞면이 나올 확률 0.5, 뒷면이 나올 확률 0.5입니다. 예측이 가장 어려운, 가장 불확실한 상태입니다.
                </p>
                <blockquote>
                    $$ H(S) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) $$<br>
                    $$ H(S) = - (0.5 \times (-1) + 0.5 \times (-1)) $$<br>
                    $$ H(S) = - (-0.5 - 0.5) = \mathbf{1.0} $$
                </blockquote>
                <p>
                    결과가 <strong>1.0</strong>입니다. 이는 불순도가 최대인 상태를 의미합니다.
                </p>

                <h4>상황 B: 조작된 동전 (완벽한 질서)</h4>
                <p>
                    앞면만 나오도록 조작된 동전이 있습니다. 앞면 확률 1.0, 뒷면 확률 0.0입니다. 던지기 전부터 결과를 100% 확신할 수 있습니다.
                </p>
                <blockquote>
                    $$ H(S) = - (1.0 \log_2 1.0 + 0 \log_2 0) $$<br>
                    (참고: $\log_2 1 = 0$ 이며, $0 \log_2 0$은 극한값으로 0으로 취급합니다)<br>
                    $$ H(S) = - (0 + 0) = \mathbf{0.0} $$
                </blockquote>
                <p>
                    결과가 <strong>0.0</strong>입니다. 불확실성이 전혀 없는, 완벽하게 순수한 상태입니다.
                </p>

                <h3>결론</h3>
                <p>
                    의사결정 나무는 분기(Split)를 할 때마다 이 <strong>엔트로피 값($H(S)$)을 계산하여, 이 값이 0에 가까워지는 방향으로</strong> 가지를 뻗어나갑니다. 즉,
                    무질서한 1.0의 상태에서 질서 정연한 0.0의 상태를 찾아가는 여정이라고 볼 수 있습니다.
                </p>
            </section>

            <section id="sec6">
                <h2>6. 수학 공식 2: 지니 계수 (Gini Impurity)</h2>

                <p>
                    엔트로피가 '정보 이론'에서 왔다면, **지니 계수(Gini Impurity)**는 원래 경제학에서 소득 불평등을 나타낼 때 쓰는 '지니 계수(Gini Coefficient)'에서
                    아이디어를 빌려온 개념입니다.
                    하지만 머신러닝에서는 **CART(Classification and Regression Trees)** 알고리즘이 채택한 불순도 측정 방법으로 더 유명합니다.
                </p>
                <p>
                    지니 계수의 직관적인 정의는 다음과 같습니다.
                </p>
                <blockquote>
                    "데이터 집합에서 임의로 두 개의 데이터를 뽑았을 때, <br>
                    <strong>두 데이터의 라벨(클래스)이 서로 다를 확률</strong>"
                </blockquote>
                <p>
                    만약 상자 안에 빨간 공만 가득하다면(순도 100%), 두 개를 뽑아도 무조건 둘 다 빨간 공일 것입니다. 즉, '서로 다를 확률'인 지니 계수는 **0**이 됩니다.
                    반대로 공이 섞여 있을수록 두 개가 다를 확률은 높아집니다.
                </p>

                <h3>지니 불순도 공식</h3>
                <p>
                    전체 클래스 개수가 $c$이고, 클래스 $i$에 속할 확률을 $p_i$라고 할 때, 공식은 다음과 같습니다.
                </p>

                <div class="formula">
                    $$ G = 1 - \sum_{i=1}^{c} p_i^2 $$
                </div>

                <p>
                    이 식은 "전체 확률(1)"에서 "두 데이터가 같을 확률($\sum p_i^2$)"을 뺀 것입니다.
                </p>


                <h3>상세 계산 시뮬레이션</h3>
                <p>
                    이해가 잘 안 되신다면, 구체적인 숫자를 대입해 단계별로 계산해 보겠습니다.
                    두 가지 클래스(A, B)가 있는 상황을 가정합니다.
                </p>

                <h4>Case 1: 완벽하게 섞인 최악의 상황 (50:50)</h4>
                <p>데이터 100개 중 A가 50개, B가 50개입니다 ($p_A = 0.5, p_B = 0.5$).</p>
                <ul>
                    <li>1단계: 각 확률을 제곱합니다. <br>
                        $0.5^2 = 0.25$, $0.5^2 = 0.25$</li>
                    <li>2단계: 제곱한 값들을 더합니다. <br>
                        $0.25 + 0.25 = 0.5$</li>
                    <li>3단계: 1에서 뺍니다. <br>
                        $1 - 0.5 = \mathbf{0.5}$</li>
                </ul>
                <p>
                    결과값 <strong>0.5</strong>는 이진 분류(Binary Classification)에서 지니 계수가 가질 수 있는 <strong>최대값</strong>입니다.
                    (엔트로피의 최대값 1.0과는 다릅니다.)
                </p>

                <h4>Case 2: 거의 분류가 된 상황 (90:10)</h4>
                <p>데이터 100개 중 A가 90개, B가 10개입니다 ($p_A = 0.9, p_B = 0.1$).</p>
                <ul>
                    <li>1단계: 제곱합니다. <br>
                        $0.9^2 = 0.81$, $0.1^2 = 0.01$</li>
                    <li>2단계: 더합니다. <br>
                        $0.81 + 0.01 = 0.82$</li>
                    <li>3단계: 1에서 뺍니다. <br>
                        $1 - 0.82 = \mathbf{0.18}$</li>
                </ul>
                <p>
                    불순도가 0.5에서 0.18로 확 떨어졌습니다. 데이터가 훨씬 깨끗해졌다는 뜻입니다.
                </p>

                <h3>엔트로피 vs 지니 계수: 무엇을 써야 할까?</h3>
                <p>
                    많은 분들이 궁금해하는 부분입니다. 결론부터 말씀드리면 <strong>"실제 성능 차이는 거의 없으나, 지니 계수가 계산이 더 빠르다"</strong>입니다.
                </p>
                <table>
                    <tr>
                        <th>비교 항목</th>
                        <th>엔트로피 (Entropy)</th>
                        <th>지니 계수 (Gini)</th>
                    </tr>
                    <tr>
                        <td><strong>계산 복잡도</strong></td>
                        <td>높음 (log 계산 필요)</td>
                        <td><strong>낮음</strong> (단순 제곱과 뺄셈)</td>
                    </tr>
                    <tr>
                        <td><strong>특징</strong></td>
                        <td>트리의 균형을 맞추려는 경향</td>
                        <td>가장 빈도 높은 클래스를 한쪽으로 고립시키는 경향</td>
                    </tr>
                    <tr>
                        <td><strong>활용</strong></td>
                        <td>C4.5, ID3 알고리즘</td>
                        <td><strong>CART, Scikit-learn 기본값</strong></td>
                    </tr>
                </table>

                <p>
                    컴퓨터 입장에서 로그($\log$) 함수를 계산하는 것은 단순 사칙연산보다 비용이 많이 듭니다.
                    데이터가 수백만 건이 넘어가면 이 작은 차이가 전체 학습 속도에 영향을 미칠 수 있습니다.
                    이러한 이유로 파이썬의 대표적인 머신러닝 라이브러리인 <strong>Scikit-learn</strong>의 `DecisionTreeClassifier`는
                    기본값(default)으로 지니 계수(`criterion='gini'`)를 사용하고 있습니다.
                </p>
            </section>

            <section id="sec7">
                <h2>7. 정보 이득 (Information Gain)</h2>

                <p>
                    앞서 배운 엔트로피나 지니 계수는 현재 상태의 '지저분한 정도'를 재는 자(Ruler)일 뿐입니다.
                    실제로 의사결정 나무가 성장하기 위해서는 <strong>"그래서 어떤 질문으로 나누어야 가장 깨끗해지는데?"</strong>라는 물음에 답해야 합니다.
                    이 물음에 대한 수학적인 해답이 바로 **정보 이득(Information Gain, IG)**입니다.
                </p>

                <h3>정보 이득의 정의</h3>
                <p>
                    정보 이득이란, 어떤 속성(Feature)으로 데이터를 분할했을 때,
                    <strong>"분할 전의 불확실성(엔트로피)에서 분할 후의 불확실성을 뺀 값"</strong>을 말합니다.
                    즉, 질문을 던짐으로써 우리가 얻게 된 정보의 양, 혹은 <strong>불순도의 감소량</strong>을 의미합니다.
                </p>
                <blockquote>
                    <strong>정보 이득(IG) = 부모의 불순도 - 자식들의 불순도 가중 평균</strong>
                </blockquote>
                <p>
                    이 값이 클수록(불순도가 많이 줄어들수록) 좋은 질문입니다. 알고리즘은 모든 가능한 질문에 대해 IG를 계산하고, 가장 값이 큰 질문을 선택합니다.
                </p>

                <h3>수학 공식과 가중 평균</h3>
                <p>
                    데이터 집합 $S$를 속성 $A$로 분할했을 때의 정보 이득 $IG(S, A)$는 다음과 같습니다.
                </p>

                <div class="formula">
                    $$ IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v) $$
                </div>

                <ul>
                    <li>$H(S)$: 분할 전 부모 노드의 엔트로피</li>
                    <li>$|S|$: 부모 노드의 전체 데이터 개수</li>
                    <li>$|S_v|$: 분할된 자식 노드($v$)의 데이터 개수</li>
                    <li>$H(S_v)$: 분할된 자식 노드의 엔트로피</li>
                </ul>

                <p>
                    여기서 중요한 것은 **$\frac{|S_v|}{|S|}$ (가중치)**입니다.
                    아무리 순도가 높은 자식 노드를 만들더라도, 그 노드에 속한 데이터가 고작 1~2개라면 전체적인 정보 이득에는 큰 기여를 하지 못하도록 설계되어 있습니다.
                    즉, <strong>"많은 데이터를 깨끗하게 분류하는 것"</strong>이 중요합니다.
                </p>

                <h3>Step-by-Step 계산 시나리오</h3>
                <p>
                    이해가 쉽도록 '소풍 가기' 예측 데이터를 예로 들어보겠습니다.
                    전체 30일 중 소풍을 간 날(Yes)이 15일, 안 간 날(No)이 15일이라고 가정해 봅시다.
                </p>

                <h4>1. 부모 노드의 엔트로피 (분할 전)</h4>
                <p>
                    정확히 반반(15 vs 15)이므로 엔트로피는 최대값입니다. <br>
                    $$ H(Parent) = 1.0 $$
                </p>

                <h4>2. '날씨'라는 질문으로 분할 시도</h4>
                <p>
                    데이터를 '날씨' 기준으로 나누었더니 다음과 같이 두 그룹이 생겼습니다.
                </p>
                <ul>
                    <li><strong>그룹 A (맑음):</strong> 10일 중 [Yes: 9, No: 1] <br>
                        &rarr; 매우 깨끗함. 엔트로피 $ \approx 0.47 $</li>
                    <li><strong>그룹 B (비):</strong> 20일 중 [Yes: 6, No: 14] <br>
                        &rarr; 다소 지저분함. 엔트로피 $ \approx 0.88 $</li>
                </ul>

                <h4>3. 자식 노드의 가중 평균 엔트로피</h4>
                <p>
                    그룹 A는 전체의 1/3(10/30)을 차지하고, 그룹 B는 2/3(20/30)를 차지합니다.
                </p>
                <blockquote>
                    $$ \text{Weighted Entropy} = ( \frac{10}{30} \times 0.47 ) + ( \frac{20}{30} \times 0.88 ) $$<br>
                    $$ = 0.157 + 0.587 = \mathbf{0.744} $$
                </blockquote>

                <h4>4. 최종 정보 이득(IG) 계산</h4>
                <p>
                    원래 엔트로피(1.0)에서 분할 후 엔트로피(0.744)를 뺍니다.
                </p>
                <blockquote>
                    $$ IG = 1.0 - 0.744 = \mathbf{0.256} $$
                </blockquote>

                <h3>결론: 알고리즘의 선택</h3>
                <p>
                    의사결정 나무(ID3 등)는 '날씨'뿐만 아니라 '온도', '습도', '바람' 등 모든 변수에 대해 위와 같은 계산을 순식간에 수행합니다.
                    만약 '습도'로 나눴을 때의 IG가 0.1이고, '날씨'의 IG가 0.256이라면,
                    알고리즘은 <strong>"날씨가 가장 중요한 질문이군!"</strong>이라고 판단하고 날씨를 루트 노드(가장 첫 번째 질문)로 배치합니다.
                </p>
            </section>

            <section id="sec8">
                <h2>8. 과적합(Overfitting)과 가지치기(Pruning)</h2>
                <h3>과적합(Overfitting)이란?</h3>
                <p>머신러닝 모델을 만들 때 가장 경계해야 할 현상 중 하나가 바로 '과적합'입니다.
                    과적합이란 모델이 학습 데이터(Training Data)를 과도하게 학습한 나머지,
                    데이터에 포함된 <strong>노이즈(Noise)나 아주 사소한 특징, 혹은 이상치(Outlier)까지 모두 패턴으로 인식하고 암기해버린 상태</strong>를 말합니다.</p>

                <p>의사결정 나무는 태생적으로 과적합에 매우 취약한 알고리즘입니다.
                    별다른 제약을 걸지 않으면, 나무는 순도(Purity)가 100%가 될 때까지,
                    즉 모든 학습 데이터를 완벽하게 구분해낼 때까지 끝없이 가지를 뻗어나갑니다.
                    결과적으로 트리의 깊이는 매우 깊어지고 구조는 지나치게 복잡해집니다.</p>

                <p><strong>비유를 들어보겠습니다.</strong></p>
                <blockquote>
                    수학 시험을 준비하는 학생이 있습니다. 이 학생이 수학의 원리(일반적인 패턴)를 이해하려 하지 않고,
                    문제집에 있는 모든 문제와 정답을 토씨 하나 안 틀리고 달달 외워버렸다고 가정해봅시다.
                    이 학생은 문제집(학습 데이터) 시험을 보면 100점을 맞겠지만,
                    숫자가 조금만 바뀐 실제 시험(테스트 데이터)에서는 0점을 맞게 될 것입니다.
                    이것이 바로 과적합입니다. </blockquote>

                <p>과적합된 모델은 <strong>'일반화(Generalization)' 능력이 떨어집니다.</strong>
                    우리가 머신러닝을 하는 이유는 과거의 데이터를 외우기 위함이 아니라,
                    미래의 새로운 데이터를 예측하기 위함입니다.
                    따라서 학습 데이터에서의 정확도가 조금 떨어지더라도,
                    새로운 데이터에서 잘 동작하는 단순하고 강건한 모델을 만드는 것이 중요합니다.</p>

                <h3>해결책: 가지치기 (Pruning)</h3>
                <p>
                    이러한 과적합을 막기 위해 나무가 끝까지 자라지 못하도록 막거나,
                    이미 자란 가지를 잘라내는 기법을 <strong>가지치기</strong>라고 합니다.
                </p>
                <ul>
                    <li><strong>사전 가지치기 (Pre-pruning):</strong> 트리를 생성하는 과정에서 성장을 미리 중단시킵니다.
                        <ul>
                            <li>트리의 최대 깊이(Max Depth) 제한</li>
                            <li>리프 노드에 있어야 할 최소 샘플 수 제한</li>
                            <li>불순도 감소량이 일정 수준 미만이면 분기 중지</li>
                        </ul>
                    </li>
                    <li><strong>사후 가지치기 (Post-pruning):</strong> 트리를 일단 완성한 후, 정보 이득이 적거나 중요도가 낮은 가지를 역으로 제거하여 일반화 성능을
                        높입니다.</li>
                </ul>

            </section>

            <section id="sec9">
                <h2>9. 장점과 단점</h2>
                <table>
                    <tr>
                        <th>장점 (Pros)</th>
                        <th>단점 (Cons)</th>
                    </tr>
                    <tr>
                        <td>이해와 해석이 쉽다 (시각화 용이)</td>
                        <td>과적합(Overfitting) 되기 쉽다</td>
                    </tr>
                    <tr>
                        <td>데이터 전처리(스케일링 등)가 거의 필요 없다</td>
                        <td>데이터의 작은 변화에도 트리가 크게 바뀔 수 있다</td>
                    </tr>
                    <tr>
                        <td>범주형, 연속형 데이터 모두 처리 가능</td>
                        <td>경계면이 수직/수평으로만 형성된다</td>
                    </tr>
                </table>
            </section>

            <section id="sec10">
                <h2>10. Python Code Example</h2>
                <p>Scikit-learn 라이브러리를 사용한 간단한 구현 예제입니다.</p>
                <pre><code>
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 1. 데이터 로드
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2
)

# 2. 모델 생성 및 학습 (가지치기: max_depth=3)
dt_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_clf.fit(X_train, y_train)

# 3. 예측 및 정확도 확인
score = dt_clf.score(X_test, y_test)
print(f"정확도: {score:.4f}")
                </code></pre>
            </section>

        </main>
    </div>

    <footer>
        <p>&copy; 2025 ML Blog Project by Student. All rights reserved.</p>
    </footer>

</body>

</html>